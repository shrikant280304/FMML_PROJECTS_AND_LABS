{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shrikant280304/FMML_PROJECTS_AND_LABS/blob/main/FMML_M5Lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjvepVoWOJU_"
      },
      "source": [
        "#Module 5 Lab 1\n",
        "\n",
        "# Non Linear Support Vector Machines\n",
        "\n",
        "```\n",
        "Module Coordinator : Nikunj Nawal\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVMs Recap:\n",
        "\n",
        "![SVM](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526288453/index3_souoaz.png)\n",
        "\n",
        "SVMs are an iterative approach of trying to find the optimal hyperplane that divides the multidimentional space into different classes present in the dataset.\n",
        "\n",
        "\n",
        "**Hyperplanes:** These are the decision planes that separate the objects of classes that we are trying to classify.\n",
        "\n",
        "**Support Vectors** : Support vectors are the points from dataset that are closest to the hyperplane that divides the dataset.\n",
        "\n",
        "**Margin**:  The gap between the closest support vectors from the different class along the direction perpendicular to the hyperplane. Simply put, it is the sum of perpendicular distance of the support vector of each class to the hyperplane.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "hq4srNYmhMOA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjCAglTcNeGi"
      },
      "source": [
        "# Importing the necessary packages\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn import svm\n",
        "\n",
        "import pandas\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffq0iFPad5dq"
      },
      "source": [
        "The topic of classifier in today's lab, SVMs make for really good linear separators. Let us look at an example which has linearly separable data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wSo9Y7DWTth"
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "ar = np.vstack(     [\\\n",
        "                    np.random.multivariate_normal(np.array([1, 1]), 1.5 * np.array([[2, -1], [-1, 2.0]]), size = 50, ),\\\n",
        "                    np.random.multivariate_normal(np.array([3, 3]), 2 * np.array([[0.75, -0.5], [-0.5, 0.75]]), size = 50, )\n",
        "                    ]\\\n",
        "              )\n",
        "\n",
        "testAr = np.vstack(   [\\\n",
        "                      np.random.multivariate_normal(np.array([1, 1]), np.array([[0.5, -0.25], [-0.25, 0.5]]), size = 500, ),\\\n",
        "                      np.random.multivariate_normal(np.array([3, 3]), np.array([[0.75, -0.5], [-0.5, 0.75]]), size = 500, )\n",
        "                      ]\\\n",
        "                  )\n",
        "testy = np.array([0] * int((testAr.shape[0]/2)) + [1] * int((testAr.shape[0]/2)))\n",
        "\n",
        "X = ar\n",
        "y = np.array([0] * int((ar.shape[0]/2)) + [1] * int((ar.shape[0]/2)))\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "def plotDecisionBoundary(X, y, pair, clf):\n",
        "  x_min, x_max = X[:, pair[0]].min() - 1, X[:, pair[0]].max() + 1\n",
        "  y_min, y_max = X[:, pair[1]].min() - 1, X[:, pair[1]].max() + 1\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                      np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "  y_pred = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "  y_pred = y_pred.reshape(xx.shape)\n",
        "  plt.figure(figsize=(5,4))\n",
        "  plt.contourf(xx, yy, y_pred, alpha=0.4)\n",
        "  plt.scatter(X[:, pair[0]], X[:, pair[1]], c = y, s = 50, edgecolor='k')\n",
        "  plt.legend([\"0\", \"1\"])\n",
        "  plt.gcf().set_dpi(130)\n",
        "  plt.show()\n",
        "\n",
        "def boundaryExp() :\n",
        "  clf = svm.LinearSVC()\n",
        "  pair = [0, 1]\n",
        "  clf.fit(X[:, pair], y)\n",
        "  plotDecisionBoundary(X, y, pair, clf)\n",
        "  plt.show()\n",
        "\n",
        "boundaryExp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtn7FxSZqRj8"
      },
      "source": [
        "# The Kernel Trick\n",
        "\n",
        "The true potential of SVMs is unleashed when they are combined with kernels.\n",
        "\n",
        "## Kernels : An intuitive explanation\n",
        "\n",
        "Kernel methods are essentially counting on using the training data (say $i^{th}$ example $(x_i, y_i)$ ) itself in a more straightforward way and learning a corresponding weight ($w_i$) for that example. Rather than trying to learn a fixed set of parameters which is done typically.\n",
        "Depending on the kind of kernel used, we can virtually project the training data in a higher dimension to make it easier for the classifier to classify them.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpzUIM1ppnxX"
      },
      "source": [
        "from sklearn.datasets import make_circles\n",
        "X, y = make_circles(200, factor=.1, noise=.1)\n",
        "\n",
        "clf = svm.SVC(kernel='linear').fit(X, y)\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='tab20')\n",
        "plt.gcf().set_dpi(110)\n",
        "plt.legend([\"0\", \"1\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cInt5fSty3w"
      },
      "source": [
        "plt.style.use(\"default\")\n",
        "clf = svm.LinearSVC()\n",
        "pair = [0, 1]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, [0, 1], clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJaPFAniusEA"
      },
      "source": [
        "However, if we artificially add another dimention to the dataset of the form:\n",
        "\n",
        "$z = x^2 + y^2$\n",
        "we can clearly see a hyperplane that can distinguish both the datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQamaPU8vfIz"
      },
      "source": [
        "Z = np.array([[i[0]**2 + i[1]**2] for i in X])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFi_pqMMvyPS"
      },
      "source": [
        "X_new = np.hstack((X, Z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JuoK_qjwPr_"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "fig = go.Figure(data = [go.Scatter3d(\n",
        "    x = X_new[:, 0],\n",
        "    y = X_new[:, 1],\n",
        "    z = X_new[:, 2],\n",
        "    mode = \"markers\",\n",
        "    marker = {\n",
        "        \"color\" : y,\n",
        "        \"line\": {\"width\" : 4, \"color\":'DarkSlateGrey'},\n",
        "        \"colorscale\": \"viridis\"},\n",
        ")])\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdGH5BF2xuab"
      },
      "source": [
        "That simple trick has helped us to get another dimension in which the data is linearly separable by a hyperplane (in this case, a 2d plane)\n",
        "\n",
        "---\n",
        "\n",
        "Now let us use the rbf kernel and use an SVM Classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybfpMPLFyIiZ"
      },
      "source": [
        "plt.style.use(\"default\")\n",
        "clf = svm.SVC(kernel='rbf')\n",
        "pair = [0, 1]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, [0, 1], clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHwwqJzHyt23"
      },
      "source": [
        "Now let us get back to our original dataset of iris and see if this kernel trick has helped us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6RS1AGtyfYs"
      },
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
        "clf = svm.SVC(kernel='poly')\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy of the decision tree on the test set: \\n\\n{:.3f}\\n\\n\".format(accuracy_score(y_pred, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29evrc-jy0WO"
      },
      "source": [
        "Certainly, using a kernel has increased our accuracy on the iris dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. To run the same experiment after filtering different features, 2 at a time.**\n"
      ],
      "metadata": {
        "id": "LteLWVYtYu8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import itertools\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Get feature names for clarity\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# List all combinations of 2 features\n",
        "feature_combinations = list(itertools.combinations(range(X.shape[1]), 2))\n",
        "\n",
        "# Iterate over each combination of 2 features\n",
        "for combo in feature_combinations:\n",
        "    # Select only the two features\n",
        "    X_filtered = X[:, combo]\n",
        "\n",
        "    # Split the dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_filtered, y, random_state=0)\n",
        "\n",
        "    # Train SVM model\n",
        "    clf = svm.SVC(kernel='poly')\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and calculate accuracy\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_pred, y_test)\n",
        "\n",
        "    # Print the result\n",
        "    feature_names_combo = [feature_names[i] for i in combo]\n",
        "    print(f\"Features: {feature_names_combo}\")\n",
        "    print(f\"Accuracy: {accuracy:.3f}\\n\")\n"
      ],
      "metadata": {
        "id": "XaGYYjZgwToJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Using different kinds of kernels for the SVM and plot it for Iris dataset -**\n",
        "1. linear\n",
        "2. poly\n",
        "3. rbf\n",
        "4. sigmoid\n",
        "5. precomputed"
      ],
      "metadata": {
        "id": "enFFTjku0c2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Standardize the data for better performance\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# List of kernels to evaluate\n",
        "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "accuracies = {}\n",
        "\n",
        "# Train and evaluate SVMs with different kernels\n",
        "for kernel in kernels:\n",
        "    clf = svm.SVC(kernel=kernel)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies[kernel] = accuracy\n",
        "    print(f\"Accuracy using {kernel} kernel: {accuracy:.3f}\")\n",
        "\n",
        "# Precomputed kernel (requires a kernel matrix)\n",
        "K_train = linear_kernel(X_train, X_train)\n",
        "clf_precomputed = svm.SVC(kernel='precomputed')\n",
        "clf_precomputed.fit(K_train, y_train)\n",
        "\n",
        "K_test = linear_kernel(X_test, X_train)\n",
        "y_pred_precomputed = clf_precomputed.predict(K_test)\n",
        "accuracy_precomputed = accuracy_score(y_test, y_pred_precomputed)\n",
        "accuracies['precomputed'] = accuracy_precomputed\n",
        "print(f\"Accuracy using precomputed kernel: {accuracy_precomputed:.3f}\")\n",
        "\n",
        "# Visualize the results in 3D using Plotly\n",
        "X_new = X_train  # You can modify this to visualize the entire dataset\n",
        "fig = go.Figure(data=[go.Scatter3d(\n",
        "    x=X_new[:, 0],\n",
        "    y=X_new[:, 1],\n",
        "    z=X_new[:, 2],\n",
        "    mode=\"markers\",\n",
        "    marker={\n",
        "        \"color\": y_train,  # Coloring based on target variable\n",
        "        \"line\": {\"width\": 4, \"color\": 'DarkSlateGrey'},\n",
        "        \"colorscale\": \"viridis\"\n",
        "    },\n",
        ")])\n",
        "\n",
        "fig.update_layout(title=\"3D Scatter Plot of Iris Dataset with SVM Kernels\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "iUgUI5L3y3OS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}